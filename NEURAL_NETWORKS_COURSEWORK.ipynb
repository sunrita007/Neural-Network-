{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T99PXtKXaeqk"
      },
      "source": [
        "**Preparing CIFAR-10 Dataset** - Calling the Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "xMZfrsnm7chG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "import torch.nn as nn\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BJKDwxg7chH"
      },
      "source": [
        "**Forming the Dataset**\n",
        "\n",
        "Resize: Resizes the images to 32x32 pixels.\n",
        "\n",
        "RandomHorizontalFlip: Randomly flips the images horizontally with a probability of 0.5.\n",
        "\n",
        "RandomAffine: Applies random affine transformations to the images, including rotation, translation, and scaling.\n",
        "\n",
        "ColorJitter: Applies random color jitter to the images, including brightness, contrast, saturation, and hue adjustments.\n",
        "\n",
        "ToTensor: Converts the images to PyTorch tensors.\n",
        "Normalize: Normalizes the tensor values by dividing by 255 (to scale the values to the range [0,1]) and then subtracting the mean and dividing by the standard deviation. The inplace=True argument modifies the tensors in place, which can save memory.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aD6a8Rb1uP1",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Define the data augmentation and normalization transformations for the training dataset\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    # Normalize the tensors by dividing by 255 (to scale the values to the range [0,1])\n",
        "    # and then subtracting the mean and dividing by the standard deviation\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], inplace=True),\n",
        "])\n",
        "\n",
        "# Define the normalization transformations for the test dataset\n",
        "transform_test = transforms.Compose([\n",
        "    # Resize the images to 32x32 pixels\n",
        "    transforms.Resize((32, 32)), \n",
        "    # Convert the images to PyTorch tensors\n",
        "    transforms.ToTensor(),\n",
        "    # Normalize the tensors by subtracting the mean and dividing by the standard deviation\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])\n",
        "\n",
        "# Download and load the training dataset\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "# Download and load the test dataset\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=50, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfA6dSDLfp59"
      },
      "source": [
        "**Visualizing a Sample** - We visualize a random subset of 10 images from the CIFAR-10 training dataset. The first step is to convert the images to a suitable format for display by reversing the normalization process and transposing them. Next, a grid of subplots is generated to present the images along with their associated labels. Each subplot displays the image and its corresponding label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEkIvyFG17QZ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Define the classes for CIFAR-10 dataset\n",
        "classes = ('frog', 'cat', 'car', 'horse', 'truck', 'deer', 'bird', 'plane', 'ship', 'dog')\n",
        "\n",
        "# Get a batch of images and labels from the trainloader\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.__next__()\n",
        "\n",
        "# Randomly select 10 images from the batch\n",
        "indices = np.random.choice(range(len(images)), size=10, replace=False)\n",
        "images = images[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "# De-normalize the images to reverse the normalization applied during data preprocessing\n",
        "images = (images * 0.5) + 0.5\n",
        "\n",
        "# Transpose the images to be in the format (height, width, channels) for display\n",
        "images = np.transpose(images, (0, 2, 3, 1))\n",
        "\n",
        "# Create a grid of subplots with 2 rows and 5 columns to display 10 images\n",
        "fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(12, 6))\n",
        "\n",
        "# Loop through each subplot and plot the corresponding image and label\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    # Plot the image in the current subplot\n",
        "    ax.imshow(images[i])\n",
        "    \n",
        "    # Get the label of the image\n",
        "    label = classes[labels[i]]\n",
        "    \n",
        "    # Set the title of the subplot as the image label\n",
        "    ax.set_title(label, fontsize=12)\n",
        "    # Remove axis markings\n",
        "    ax.axis('off')\n",
        "\n",
        "# Show the plot with the grid of images\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mPa7hJd6rYg"
      },
      "source": [
        "**Creating the Model** - This code defines a custom neural network block called \"Block\" that performs adaptive convolutions based on channel-wise weights. The block contains multiple convolutional layers, with weights calculated using a fully connected layer that takes input from an adaptive average pooling layer. The block also has a residual connection to enhance the learning of the network.\n",
        "\n",
        "The benefits  are that it can adaptively adjust the weights of the convolutional layers based on the input data, allowing for more efficient and effective feature extraction. The residual connection also allows for better training of the network by helping to prevent vanishing gradients. This block can be used as a building block for constructing more complex neural network architectures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hp1CkA8d8qE0",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, k=3):\n",
        "        super(Block, self).__init__()\n",
        "        # Adaptive average pooling to reduce the spatial dimensions to 1x1\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        # Fully connected layer to obtain channel-wise weights for adaptive convolutions\n",
        "        self.fc = nn.Linear(in_channels, k)\n",
        "        \n",
        "        # Convolutional layers for the block\n",
        "        self.convs = nn.ModuleList()\n",
        "        for _ in range(k):\n",
        "            conv = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
        "            # Initialize the weights of the convolutional layer\n",
        "            nn.init.kaiming_normal_(conv.weight, mode='fan_in', nonlinearity='relu')\n",
        "            self.convs.append(conv)\n",
        "            \n",
        "        # Residual connection\n",
        "        self.residual = nn.Identity()\n",
        "        if in_channels != out_channels:\n",
        "            self.residual = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "            # Initialize the weights of the residual connection\n",
        "            nn.init.kaiming_normal_(self.residual[0].weight, mode='fan_in', nonlinearity='relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        # Calculate the channel-wise weights using the fully connected layer\n",
        "        a = self.fc(self.avg_pool(x).view(b, c))\n",
        "        a = F.softmax(a, dim=1).view(b, -1, 1, 1)\n",
        "\n",
        "        # Perform the adaptive convolutions with the calculated weights\n",
        "        out = 0\n",
        "        for i, conv in enumerate(self.convs):\n",
        "            out += a[:, i:i + 1] * conv(x)\n",
        "        \n",
        "        # Add the residual connection to the output\n",
        "        res = self.residual(x)\n",
        "        out += res\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model**\n",
        "The code defines a neural network model called CIFAR10_NN for the CIFAR-10 dataset. The model consists of a backbone that is made up of a series of Block instances that perform adaptive convolutions based on channel-wise weights. Each Block contains multiple convolutional layers, and the weights are calculated using a fully connected layer with input from an adaptive average pooling layer. The model also has residual connections to enhance the learning of the network. The backbone is followed by a classifier that maps the output of the backbone to the final class scores.\n",
        "\n",
        "The alternative approach could have been to use a pre-trained model and fine-tune it for the CIFAR-10 dataset. This approach would have been faster than training the model from scratch as the pre-trained model has already learned useful features on a large dataset. One popular pre-trained model for computer vision tasks is the ResNet model, which has shown to achieve state-of-the-art performance on various image classification tasks."
      ],
      "metadata": {
        "id": "BixAXuJcFQ-7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "6k0UzbOq7chK"
      },
      "outputs": [],
      "source": [
        "class CIFAR10_NN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CIFAR10_NN, self).__init__()\n",
        "        # Define the backbone of the model as a sequence of blocks and other layers\n",
        "        self.backbone = nn.Sequential(\n",
        "            Block(3, 64),                # Change the number of input channels from 3 to 32\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            Block(64, 64),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            Block(64, 128),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            Block(128, 128),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            Block(128, 128),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            Block(128, 256),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            Block(256, 256),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            Block(256, 256),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            Block(256, 512),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            Block(512, 512),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            Block(512, 512),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),  # Reduce the spatial dimensions to 1x1\n",
        "            nn.Flatten(),                 # Flatten the tensor into a 1D vector\n",
        "            nn.Linear(512, 10)            # Fully connected layer to produce the final class scores\n",
        "        )\n",
        "\n",
        "    # Define the forward pass of the model\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)          # Pass the input through the backbone\n",
        "        x = self.classifier(x)        # Pass the output of the backbone through the classifier\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gwmFfU57chK"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the CIFAR10Model class\n",
        "model = CIFAR10_NN()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss Function**\n",
        "This code first sets the device to be used for computation to either GPU (if available) or CPU. Then, it defines a custom loss function called CustomLoss that modifies the standard cross-entropy loss by applying a custom modification. The modification involves raising (1 - e^-cross_entropy_loss) to the power of gamma, multiplying it with the alpha and then multiplying the product with the cross-entropy loss.\n",
        "\n",
        "The forward method of the CustomLoss class takes two inputs - input and target - and computes the custom loss by first computing the cross-entropy loss using PyTorch's built-in F.cross_entropy() function. If the target tensor is one-hot encoded, it is converted to class labels. If the target tensor has an invalid number of dimensions, an error is raised. Finally, the custom loss is computed using the modification described above and returned.\n",
        "\n",
        "This custom loss function can be used as a replacement for the standard cross-entropy loss function in training neural networks. The custom modification can potentially help the model converge faster or better by emphasizing hard-to-predict examples."
      ],
      "metadata": {
        "id": "lkgm5t7tFjtH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjrEzhi57chK"
      },
      "outputs": [],
      "source": [
        "# Set the device to use (GPU if available, else CPU)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "class CustomLoss(nn.Module):\n",
        "    def __init__(self, alpha=1, gamma=2):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        \n",
        "    def forward(self, input, target):\n",
        "        # Check if the target tensor is one-hot encoded\n",
        "        if target.ndim == input.ndim:\n",
        "            target = torch.argmax(target, dim=1)\n",
        "        elif target.ndim != input.ndim - 1:\n",
        "            raise ValueError(f\"Target tensor has invalid dimensions: expected {input.ndim - 1}, got {target.ndim}\")\n",
        "        \n",
        "        # Compute the cross-entropy loss\n",
        "        ce_loss = F.cross_entropy(input, target, reduction='mean')\n",
        "        \n",
        "        # Apply a custom modification to the cross-entropy loss\n",
        "        custom_loss = self.alpha * (1 - torch.exp(-ce_loss)) ** self.gamma * ce_loss\n",
        "        \n",
        "        return custom_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimizer and learning rate scheduler** are important components in training a neural network.\n",
        "\n",
        "The optimizer specifies the algorithm to use for updating the weights of the neural network during training to minimize the loss function. In this code, the Adam optimizer is used with a learning rate of 0.001. Adam is a popular optimizer that combines the advantages of two other optimizers: AdaGrad and RMSProp.\n",
        "\n",
        "The learning rate scheduler adjusts the learning rate during training to improve performance. In this code, a cosine annealing scheduler is used with a maximum number of epochs of 50 and a minimum learning rate of 1e-6. Cosine annealing is a technique that gradually reduces the learning rate over time in a cyclical manner, where the learning rate decreases from a maximum value to a minimum value and then increases again. This can help improve the model's ability to converge to a better solution by allowing it to explore the solution space more effectively."
      ],
      "metadata": {
        "id": "i8yzoxDTF0EP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CX6T7Q0y7chL"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the focal loss function\n",
        "criterion = CustomLoss()\n",
        "\n",
        "# Define optimizer for the model with new hyperparameters\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "# Define learning rate scheduler for the optimizer with new hyperparameters\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a function train_epoch that trains a PyTorch model for one epoch (i.e., one pass through the training data).\n",
        "\n",
        "The function takes in the following arguments:\n",
        "\n",
        "**model:** the PyTorch model to train(CFIR10_NN)\n",
        "\n",
        "**dataloader:** a PyTorch DataLoader object containing the training data (images and labels)\n",
        "\n",
        "**criterion:** the loss function to optimize during training\n",
        "\n",
        "**optimizer:** the optimization algorithm to use during training\n",
        "\n",
        "**device:** the device (CPU or GPU) to use for training\n",
        "\n",
        "**accumulation_steps: **the number of mini-batches to accumulate gradients over before updating the model parameters\n",
        "\n",
        "The function performs the following steps:\n",
        "\n",
        "Sets the model to training mode using model.train().\n",
        "1. Initializes variables for keeping track of the running loss, number of correct predictions, and total number of samples.\n",
        "2. Resets the gradients using optimizer.zero_grad().\n",
        "3. Loops through the training data (images and labels) using a for loop.\n",
        "4. Moves the images and labels to the specified device using images.to(device) and labels.to(device).\n",
        "5. Performs a forward pass through the model using outputs = model(images).\n",
        "6. Calculates the loss between the predicted outputs and the true labels using the specified loss function criterion(outputs, labels).\n",
        "7. Divides the loss by the specified accumulation_steps.\n",
        "8. Performs backpropagation to compute gradients using loss.backward().\n",
        "9. Accumulates the loss using running_loss += loss.item() * accumulation_steps.\n",
        "10. Gets the predicted class labels using _, predicted = outputs.max(1).\n",
        "11. Updates the total number of samples using total += labels.size(0).\n",
        "Counts the number of correct predictions using correct += predicted.eq(labels).sum().item().\n",
        "12. Updates the model parameters after accumulating gradients for accumulation_steps mini-batches using optimizer.step() and resets the gradients using optimizer.zero_grad().\n",
        "13. Calculates the average loss and accuracy for the epoch using epoch_loss = running_loss / len(dataloader.dataset) and epoch_acc = correct / len(dataloader.dataset).\n",
        "14.Returns the epoch loss and accuracy as a tuple."
      ],
      "metadata": {
        "id": "PxFjFTrsGLaP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1oN50ufHuaW",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, criterion, optimizer, device, accumulation_steps=4):\n",
        "    model.train()  # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    optimizer.zero_grad()  # Reset gradients outside the loop\n",
        "  \n",
        "    # Loop through the training data (images and labels)\n",
        "    for i, (images, labels) in enumerate(dataloader):\n",
        "        images, labels = images.to(device), labels.to(device)  # Move images and labels to the device\n",
        "        outputs = model(images)  # Forward pass through the model\n",
        "        loss = criterion(outputs, labels)  # Calculate the loss\n",
        "        loss /= accumulation_steps\n",
        "        loss.backward()  # Perform backpropagation to compute gradients\n",
        "\n",
        "        # Update model parameters after accumulating gradients for 'accumulation_steps' mini-batches\n",
        "        if (i + 1) % accumulation_steps == 0:\n",
        "            optimizer.step()  # Update the model parameters\n",
        "            optimizer.zero_grad()  # Reset the gradients\n",
        "\n",
        "        running_loss += loss.item() * accumulation_steps  # Accumulate the loss\n",
        "        _, predicted = outputs.max(1)  # Get the predicted class labels\n",
        "        total += labels.size(0)  # Update the total number of samples\n",
        "        correct += predicted.eq(labels).sum().item()  # Count the number of correct predictions\n",
        "\n",
        "    # Update model parameters after accumulating gradients for the last mini-batch\n",
        "    if i % accumulation_steps != accumulation_steps - 1:\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)  # Calculate the average loss for the epoch\n",
        "    epoch_acc = correct / len(dataloader.dataset)  # Calculate the accuracy for the epoch\n",
        "\n",
        "    return epoch_loss, epoch_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **validate_epoch** function is used to evaluate the performance of a trained model on a validation set. The function sets the model to evaluation mode, computes the validation loss and accuracy for the given dataloader using the criterion provided. It returns the average validation loss and accuracy for the epoch. The function does not perform backpropagation or update the model parameters."
      ],
      "metadata": {
        "id": "yOWBJiR9HACC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m231vxs57chL"
      },
      "outputs": [],
      "source": [
        "def validate_epoch(model, dataloader, criterion, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in dataloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)  # Move inputs and labels to the device\n",
        "\n",
        "        with torch.no_grad():  # Disable gradient calculation\n",
        "            outputs = model(inputs)  # Forward pass through the model\n",
        "            loss = criterion(outputs, labels)  # Calculate the loss\n",
        "\n",
        "        running_loss += loss.item()  # Accumulate the loss\n",
        "\n",
        "        _, predicted = torch.max(outputs, dim=1)  # Get the predicted class labels\n",
        "        total += labels.size(0)  # Update the total number of samples\n",
        "        correct += (predicted == labels).sum().item()  # Count the number of correct predictions\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader)  # Calculate the average loss for the epoch\n",
        "    epoch_acc = correct / total  # Calculate the accuracy for the epoch\n",
        "\n",
        "    return epoch_loss, epoch_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tghy3LNfl36G"
      },
      "source": [
        "**Training the Model and Printing Results** - We train the model for a specified number of epochs and visualize the loss and accuracy of both training and validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2ktPMhlimMZ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Training parameters\n",
        "num_epochs = 35\n",
        "print_every = 7\n",
        "\n",
        "# Store the loss and accuracy history\n",
        "train_loss_h = []\n",
        "val_loss_h = []\n",
        "train_acc_h = []\n",
        "val_acc_h = []\n",
        "\n",
        "start = time.time()\n",
        "end = time.time()\n",
        "\n",
        "# Loop through each epoch\n",
        "for epoch in range(num_epochs):\n",
        "    # Train the model for one epoch and calculate training loss and accuracy\n",
        "    train_loss, train_acc = train_epoch(model, trainloader, criterion, optimizer, device)\n",
        "    # Validate the model and calculate validation loss and accuracy\n",
        "    val_loss, val_acc = validate_epoch(model, testloader, criterion, device)\n",
        "\n",
        "    # Update the learning rate scheduler at every epoch\n",
        "    scheduler.step()\n",
        "\n",
        "    # Store the calculated losses and accuracies for the current epoch\n",
        "    train_loss_h.append(train_loss)\n",
        "    val_loss_h.append(val_loss)\n",
        "    train_acc_h.append(train_acc)\n",
        "    val_acc_h.append(val_acc)\n",
        "\n",
        "    # Print the losses and accuracies for the current epoch if it's a multiple of 'print_every'\n",
        "    if (epoch + 1) % print_every == 0:\n",
        "        print('Epoch [{}/{}], Train Loss: {:.4f}, Train Acc: {:.4f}, Val Loss: {:.4f}, Val Acc: {:.4f}'.format(epoch+1, num_epochs, train_loss, train_acc, val_loss, val_acc))\n",
        "\n",
        "\n",
        "print(f'Training finished in {(end - start):.2f} seconds.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plotting Results**"
      ],
      "metadata": {
        "id": "dtI2OQocHH1k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "j7dbsiV37chM"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "axs[0].plot(train_loss_h, label='Training Loss')\n",
        "axs[0].plot(val_loss_h, label='Validation Loss')\n",
        "axs[0].set_xlabel('Epoch')\n",
        "axs[0].set_ylabel('Loss')\n",
        "axs[0].set_title('Loss Curves')\n",
        "axs[0].legend()\n",
        "\n",
        "axs[1].plot(train_acc_h, label='Training Accuracy')\n",
        "axs[1].plot(val_acc_h, label='Validation Accuracy')\n",
        "axs[1].set_xlabel('Epoch')\n",
        "axs[1].set_ylabel('Accuracy')\n",
        "axs[1].set_title('Accuracy Curves')\n",
        "axs[1].legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in testloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = 100 * correct / total\n",
        "print('Accuracy of the model on the test set: {:.2f}%'.format(accuracy))"
      ],
      "metadata": {
        "id": "oMvrlNzYV2M-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjQO0BDEg-vb"
      },
      "source": [
        "**Validation Set Visualization** - In this section, we visualize a random subset of the validation set with predicted and actual labels. The purpose of this visualization is to give an intuitive understanding of the performance of our model on the validation set. The images are displayed along with their corresponding predicted and actual labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmWD6W3Dg-vb",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Create empty lists to store the predictions and actual labels\n",
        "preds = []\n",
        "targets = []\n",
        "\n",
        "# Loop through the validation data and collect predictions and actual labels\n",
        "with torch.no_grad():\n",
        "    for images, labels in testloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        preds.append(predicted.cpu().numpy())\n",
        "        targets.append(labels.cpu().numpy())\n",
        "\n",
        "# Concatenate the lists of predictions and actual labels into numpy arrays\n",
        "preds = np.concatenate(preds)\n",
        "targets = np.concatenate(targets)\n",
        "\n",
        "# Visualize a random subset of the validation set with predicted and actual labels\n",
        "fig = plt.figure(figsize=(25, 8))\n",
        "fig.suptitle(\"Random Subset of the Validation Set with Predicted and Actual Labels\", fontsize=20)\n",
        "\n",
        "for i in range(20):\n",
        "    index = np.random.randint(0, len(preds))\n",
        "    ax = fig.add_subplot(4, 10, i+1, xticks=[], yticks=[])\n",
        "    ax.imshow(testset.data[index])\n",
        "    ax.set_title(f\"Predicted: {classes[preds[index]]}\\nActual: {classes[targets[index]]}\", color=(\"green\" if preds[index]==targets[index] else \"red\"), fontsize=12)\n",
        "    \n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(top=0.85)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**References: **\n",
        "\n",
        "\n",
        "1. How adaptive pooling works in PyTorch https://stackoverflow.com/questions/53841509/how-does-adaptive-pooling-in-pytorch-work\n",
        "\n",
        "2. GitHub repository with an example implementation of a custom model in PyTorch: https://github.com/ejcgt/attention-target-detection/blob/master/model.py\n",
        "\n",
        "3. PyTorch forum discussion on adding layers after defining a model: https://discuss.pytorch.org/t/adding-layers-after-defining-the-model/141859\n",
        "\n",
        "4. PyTorch forum discussion on the meaning of \"return nn.Sequential(*layers)\": https://discuss.pytorch.org/t/the-meaning-of-return-nn-sequential-layers/93070\n",
        "\n",
        "5. Analytics Vidhya article on writing a custom loss function in TensorFlow: https://www.analyticsvidhya.com/blog/2022/09/dummies-guide-to-writing-a-custom-loss-function-in-tensorflow/\n",
        "\n",
        "6. CNVrg article on creating custom loss functions in Keras: https://cnvrg.io/keras-custom-loss-functions/\n",
        "\n",
        "7. https://pytorch.org/docs/stable/optim.html#torch.optim.Adam\n",
        "\n",
        "8. PyTorch documentation on the Adam optimizer: https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/\n",
        "\n",
        "9. Blog post on the Adam optimization algorithm for deep learning: https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.CosineAnnealingLR\n",
        "\n",
        "10. PyTorch documentation on the Cosine Annealing learning rate scheduler : https://towardsdatascience.com/understanding-learning-rate-schedules-and-adaptive-learning-rate-methods-in-deep-learning-7fc642831215\n",
        "\n",
        "11. Blog post on understanding learning rate schedules and adaptive learning rate methods in deep learning: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#training-the-model\n",
        "\n",
        "12.  Optimizers: https://towardsdatascience.com/how-to-train-neural-network-faster-with-optimizer-buffers-7f58d19fc65e\n",
        "\n",
        "13. Improving Accuracy https://medium.com/@stepanulyanin/improving-the-accuracy-of-your-deep-learning-model-using-accumulation-steps-5a3bd1f504db \n",
        "\n",
        "14. Kaggle competition on image classification: https://www.kaggle.com/c/cifar-10\n",
        "This Kaggle competition provides a dataset for image classification, similar to the one used in the code. You can use this competition to test your skills in image classification and learn from the solutions provided by other participants."
      ],
      "metadata": {
        "id": "5KmFzMBDBCg1"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}